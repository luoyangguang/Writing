\section{Related works}

%Trust and Privacy Enabled Service Composition using Social Experience
%Trust aware approaches for Web service composition have been investigated widely in the literature.

%reputation based approach
%Kuter and Golbeck [1] targeted OWL-S upper ontology and followed a reputation based approach for selecting highly trusted composite web service.

%multi-agent based reputation model
%Paradesi et al. [15] adopted a multi-agent based reputation model to define trustworthiness of services.
%Moreover,
%	they developed a trust framework to derive trust for a composite service from trust model of component services.
%Unlike our work,
%	none of the aforementioned trust aware approaches considered privacy of users when they infer trust relationships or exploit their profile content.

%privacy score
Liu and Trezi [19] developed mathematical models to estimate the privacy score of the disclosed information by online social network users,
	mathematical models are based on visibility and sensitivity of the individual items in user profile.
Unlike our model,
	their approach do not support personalized privacy view over profile content for each individual in the social network.

%%Trust-involved access control in collaborative open social networks

%Trust in a person is a commitment to an action based on a belief that the future actions of that person will lead to a good outcome [Golbeck,2009].
%There are three main properties of trust that are relevant to the development of algorithms for computing it,
%	namely,
%	transitivity,
%	asymmetry,
%	and personalization.

%%transitivity
%The primary property of trust that is used in our work is transitivity.
%	if Alice highly trusts Bob,
%	and Bob highly trusts Chuck,
%	it does not always and exactly follow that Alice will highly trust Chuck.
%%asymmetry
%It is also important to note the asymmetry of trust,
%	for two people involved in a relationship,
%	trust is not necessarily identical in both directions.
%%personalization
%The third property of trust that is important in social networks is the personalization of trust,
%	trust is inherently a personal opinion,
%	two people often have very different opinions about the trustworthiness of the same person.

%%collaborative community
%A possible solution is to estimate the trust level to be assigned to a user in a collaborative community on the basis of his/her reputation,
%	given by his/her behavior with regards to all the other users in the community.
%In our scenario,
%	this can be done by making a user able to monitor the behavior of the other users wrt the release of private

%%Social Market: Combining Explicit and Implicit Social Networks

%SybilGuard
SybilGuard [23] and SybilLimit [22] propose protocols that exploit trust relationships between friends to protect peer-to-peer systems from sybil attacks.

%Reliable Email
Reliable Email [9] propose protocols that exploit trust relationships between friends to build an email-whitelisting system based on friend-to-friend relationships.

%%Ostra
%Ostra [18] exploits social trust to limit the incidence of unwanted communication in messaging and content-sharing systems.

%%NABT
%NABT [15] proposes the use of trust between friends to prevent free-riding behaviors using an indirect trust relationships,
%	NABT’s credit-based approach can be viewed as a basic form of trust inference between friends of friends.
%A more advanced approach to trust-inference is adopted by SUNNY [14],
%	a centralized protocol that takes into account both trust and confidence to build a Bayesian network.

%%TrustWalker
%TrustWalker [10] combines trust and item-based collaborative filtering.

%%TaRS
%TaRS [17] builds a recommendation system capable of operating both with global and with local trust metrics.

%%Global trust metrics
%Global trust metrics [7] predict a global reputation value for each node.

%%Local trust metrics
%Local trust metrics [16],
%	on the other hand,
%	take an approach similar to ours and compute trust values that are dependent on the target user.

%%Social Market
%Despite the mole of work on social trust,
%	Social Market is,
%	to the best of our knowledge,
%	the first system to propose the use of trust relationships to build a decentralized interest-based marketplace.
%Similarly,
%	TAPS is the first attempt to combine explicit and implicit social networks into a single gossip protocol.

%%gossip protocols
%Existing research on gossip protocols has addressed a number of problems including data dissemination [5],
%	aggregation [13].
%In this context,
%	the two contributions that are most closely related to our work are [8],
%	which uses gossip to disseminate news in explicit networks,
%	and [4], which proposed the use of gossip for implicit ones.

%%LENS: Leveraging Social Networking and Trust to Prevent Spam Transmission

Recently,
	several techniques have been proposed using social networks and trust and reputation systems to combat spam.

%%Ostra
%Ostra [9] utilizes trust relationship to thwart unwanted communication,
%	where the number of a user’s trust relationships is used to limit the amount of unwanted communications he can produce.
%Ostra relies on existing trust networks to connect senders and receivers via chains of pair-wise trust relationship and use a pair-wise,
%	link-based credit scheme to impose a cost on originator of unwanted communication.
%However,
%	its scalability of this system stays uncertain as it employs a per-link credit scheme.

%Reliable Email
Re: Reliable Email [5] exploits the use of white list of friends and automatic white list of FoF to increase the communication chance of only white list friends.
By using this protocol,
	RE can accept almost 85\% of received emails and prevent up to 88\% false positive by the existing spam filters.
However,
	emails other then friends and FoF still had to be filtered by noisy and unreliable spam filters.

%%SOAP
%SOAP [8] presents a social network based personalized spam filter that integrates social closeness,
%	user (dis)interest and adaptive trust management into a Bayesian filter.
%However,
%	several issues with SOAP,
%	including the intrinsic cost of initialization and continuous adaptation of social closeness (between sender and recipient) 
%		and social interests (of an individual) in the Bayesian filter,
%	limit its usage.

%%SocialFilter
%SocialFilter [10] proposes a collaborative spam mitigation system that uses social trust embedded in OSN to asses the trustworthiness of Spam reporter.
%The spammer reports from the SocialFilter nodes are stored at a centralized repository that computes the trust values of the reports and identifies spammers based on IP addresses.
%However,
%	the SocialFilter’s effectiveness is doubtful as spammers may use dynamic IPs.

In Trust and Reputation Systems,
	network users try to calculate the reliability and trustworthiness of other users based on their own experiences and that of others.
Boykin et al. [3] proposed an automatic email ranking system based on trust and reputation algorithms.
MailRank [4] is a spam detection system based on trust and reputation scheme to classify email addresses

%%SNARE
%SNARE [6] infers the reputation of an email sender solely based on network-level features,
%	without looking at the contents of a message.
%Using an automated reputation engine,
%	SNARE classifies email senders as spammers or legitimate with about a 70\% detection rate for less than a 0.3\% false positive rate.
%However,
%	lacking authentication and non-repudiation in standard trust and reputation solution make these solutions be subject to identity spoofing,
%	false accusation and collusion attacks.
%Further,
%	these solutions consume extra valuable resources of email servers on email reception and filtering.

%lens
In contrast,
	LENS can reject unwanted email traffic during the SMTP time.

%%%Detecting and Resolving Privacy Conflicts for collaborative Data Sharing in Online Social Networks

%Several proposals of an access control scheme for OSNs have been introduced.

%%trust-based access control mechanism
%Carminati et al. [9] introduced a trust-based access control mechanism,
%	which allows the specification of access rules for online resources where authorized users are denoted in terms of the relationship type,
%	depth,
%	and trust level between users in OSNs.
%They further presented a semi-decentralized discretionary access control system and a related enforcement mechanism for controlled sharing of information in OSNs [10].
%However,
%	none of these work could accommodate privacy control requirements with respect to the collaborative data sharing in OSNs.

%collective privacy management
Squicciarini et al. [22] proposed a solution for collective privacy management for photo sharing in OSNs.
This work considered the privacy control of a content that is co-owned by multiple users in an OSN,
	such that each co-owner may separately specify her/his own privacy preference for the shared content.
The Clarke-Tax mechanism was adopted to enable the collective enforcement for shared content.
Game theory was applied to evaluate the scheme.
However,
	a general drawback of this solution is the usability issue,
	as it could be very hard for ordinary OSN users to comprehend the Clarke-Tax mechanism and specify appropriate bid values for auctions.
In addition,
	the auction process adopted in their approach indicates only the winning bids could determine who was able to access the data,
	instead of accommodating all stakeholders’ privacy preferences.
In our work,
	we introduce an effective conflict resolution solution,
	which makes a tradeoff between privacy protection and data sharing considering the privacy concerns from multiple associated users.

%PrivAware
Becker et al. [6] presented PrivAware,
	a tool to detect and report unintended information loss through quantifying privacy risk associated with friend relationship in OSNs.

%privacy protection tool
In [23],
	Talukder et al. discussed a privacy protection tool,
	called Privometer,
	which can measure the risk of potential privacy leakage cased by malicious applications installed in the user’s friend profiles,
	they suggest self-sanitization actions to lessen this leakage accordingly.

%privacy score
Liu et al. [20] proposed a framework to compute the privacy score of a user,
	indicating the user’s potential risk caused by her/his participation in OSNs.
Their solution also focused on the privacy settings of users with respect to their profile items.
Compared with those existing work,
	our approach measures the privacy risk caused by different privacy concerns from multiple users,
	covering profile sharing,
	friendship sharing,
	as well as content sharing in OSNs.

%%Measurement and evaluation of a real world deployment of a challenge-response spam filter

Most of the spam blocking techniques proposed by previous research fall into two categories: content-based and sender-based filtering.

%Content-based spam filtering techniques
Content-based spam filtering techniques rely on signatures and classification algorithms applied to the emails content [32, 14, 19, 15, 33].
Although content-based filters were initially very effective and provided an acceptable defense against spam [27],
	with the evolution of the spam sophistication they became less effective over time.

%Sender-based spam filtering techniques
Sender-based spam filtering techniques aim instead at blocking spams by analyzing information about the sender.
To date,
	a wide range of sender-based filtering solutions has been proposed,
	including sender authentication (e.g.,
		SPF [36],
		DMIK [18],
		and Occam [22]),
	sender IP reputation [11],
	network-level feature categorization,
	sender whitelisting,
	and detecting anomalies on the sender’s behavior.

%Sender authentication techniques
Sender authentication techniques authenticate the sender either by verifying it’s domain [36, 18],
	or by providing a protocol to authenticate the server at each message delivery [22].
However,
	these kind of solutions are quite effective to prevent email spoofing,
	a phenomenon that is very common among spammers.

%sender IP reputation
Approaches based on IP reputation [11, 25] rely on whitelists or/and blacklists of IP addresses that are known either to send spam or to be a trusted source.
Therefore,
	these approaches are effective against static spammers and openrelay servers used for spam distribution.
However,
	they are not able to provide the same degree of protection against spam sent through botnets,
	since botnets can change the sender’s IP address by using a large number of different infected machines to deliver the messages.

%Behavior-based solutions
Behavior-based solutions,
	like the ones proposed by Pathak et al. [30] or Ramachandran et al. [31],
	and network-level detection techniques,
	like the one proposed by Hao et al. [24],
	tend to react faster to spam campaigns and have a lower number of false-positive.
However,
	these kind of solutions block only part of illegitimate emails,
	and, therefore,
	they have to be used in combination with other filters.

%challenge-response technique
One of the most wide-spread approaches for building and maintaining a list of trusted senders is based on the adoption of a challenge-response technique [28, 29, 8, 12, 6],
	already largely described in the rest of the paper.
To the best of our knowledge,
	the only empirical study that analyzes challenge-response based whitelisting systems is presented by Erickson et.al. [21].
The results of their evaluation support the usability of CR systems,
	but also show their limitations in coping with automatically generated legitimate emails,
	such as newsletters and notifications.

Our work aims instead to present a comprehensive study of a real-world whitelisting challenge-response antispam system,
	evaluating it’s effectiveness and it’s impact on the endusers,
	Internet,
	and server administration.

%Measuring Privacy

Privacy research is multi-disciplinary.
Like trust,
	the concept of privacy originates in the field of psychology and social psychology.
Hirshleifer writes about privacy as an evolutionary trait,
	were sociality is based on the principles of sharing,
	private rights and dominance.

Jacobs and Abowd [4] propose a new framework for technologists to consider privacy requirements,
	which makes a unique contribution in computer science by positioning the individual (as opposed to the system) front and centre.
The critical points are: consideration of the physical nature of the input stimulus,
	location origin,
	sensing location and granularity of information produced.
This combination of hardware,
	software and usage factors are the basis of the proposed framework,
	which is developed based on the Terrell’s legal ethics work,
	itself rooted in meta ethics (how values are expressed rather than what they are).

%Privacy leakage vs. Protection measures: the growing disconnect

We now enumerate existing privacy protection measures available to users and the two new protection proposals.

%Blocking requests to targeted third parties
Blocking requests to targeted third parties: This block measure includes using 
	an advertisement blocking tool (AdBlock Plus [1]),
	or a browser builtin (IE9 [2]),
	to syntactically block selected third parties via server/domain name.
Another measure blockhidden [15] determines the true source of hidden third-parties by examining their authoritative DNS servers.

%Refusing cookies to prevent tracking
Refusing cookies to prevent tracking: Browsers can be set to refuse all cookies (nocook) or just third-party cookies (no3rdcook).

%Disabling script execution
Disabling script execution: JavaScript execution can be disabled (nojs) either permanently via the browser or selectively via a tool such as NoScript [17].

%Filtering protocol headers
Filtering protocol headers: 
This is done via extensions or at an intermediary and includes the referrer measure available in some browsers to modify or remove the Referer header in an HTTP request.

%Anonymizing the user and user action
Anonymizing the user and user actions: One such anon measure is anonymizing user’s IP address via an anonymizing proxy or by using Tor (https://www.torproject.org/).

%Opting out of tracking
Opting out of tracking:
This can be done via optout to evade tracking (via cookies) by an aggregator using tools such as the Firefox TACO extension [21] that sets persistent opt-out cookies.
Unfortunately some aggregators continue to track when the cookie is present and just not use the information to serve targeted ads [19].

%Do-Not-Track HTTP header proposal
Do-Not-Track HTTP header proposal:
Researchers proposed
	in early 2010 
	that browsers add a HTTP DoNot-Track-Header (DNT-Header) [5] to allow users to express their interest in not being tracked by any aggregator or ad network.
However,
	the extent to which third parties would honor such a header is unknown.

%FTC consumer privacy protection proposal
FTC consumer privacy protection proposal: The U.S. Federal Trade Commission released a report [10] in December 2010,
	aimed at policymakers and the industry-stating that companies do not adequately address consumer privacy concerns and that information (such as privacy policies) 
		and choices (various privacy settings) available to users are confusing.
The report was based on three meetings held by the FTC in which privacy activists,
	researchers,
	technologists,
	and aggregator company representatives participated.
Noting the potential benefits to users about information flow it pointed out the asymmetricity with respect to the low cost of invisible data collection and potential harm to consumers.

Additionally,
	off-line information is being increasingly linked with on-line tracking data leading to easier identification of users.

%%COAT : Collaborative Outgoing Anti-Spam Technique

%Current popular methods for mitigating spam can be categorized into one of the three classes based on their approach,
%	contents analysis [7],
%	sender reputation [8, 9, 5, 10],
%	and community collaboration [4, 6].

%%Content based filtering
%Content based filtering [7] is the most common technique for filtering spam on the basis of contents they contain and can be divided into two types.

%	%rules
%	The first is done by defined rules and used when all classes are static,
%		and their components are easily separated according to some features.
%	The typical example is the rule based expert systems.

%	%machine learning techniques
%	The second type is done using machine learning techniques and used when the characteristics are not well defined.
%	These techniques attempt to generate on a set of samples,
%		quasi or semi automatically a classifier with an acceptable error rate.

%%sender reputation
%	The second class of anti-spamming classifies email based upon who is sending rather than what the contents are.

%	%blacklist
%	A blacklist [8] maintains the listing of problematic hosts and do not receive emails from them.

%	%whitelist
%	An email whitelist [9] identifies the people one accepts email from,
%		this includes friends,
%		family,
%		and other contacts.

%%community collaboration
%The collaborative systems [4] do not rely upon semantic analysis but on the community to identify spam messages.
%Once a message is tagged as spam by one SMTP server,
%	the signature of that message is transmitted to all other SMTP servers.
%This class requires the collaboration of multiple SMTP servers to implement the system.

%%COAT
%The proposed work differs from the other techniques in a way that all of them categorize mail messages at receiver side,
%	whereas COAT works at the sender side and reduces outgoing spam rather than inbox spam.
%We have hardly found any work in literature about saving the Internet bandwidth and resource wastage by spam.

%A Privacy Awareness System for Facebook Users

The large expansion of social networks and the exchange of vast amounts of personal and private information have been facilitated to an extraordinary degree due to almost ubiquitous Internet access.
Several works aim to understand the significance of the risks that are involved by focusing on examining identity leakage due to individual attacks or unwanted data mining [5, 15].

%Protect_ U & Privacy Wizard
In order to protect social network users,
	many studies,
	such as Protect\_ U,
	applied the Privacy Feedback and Awareness (PFA) [16] approach.
This approach aims to enhance users’ understanding of the privacy implications of their system use.
Moreover,
	it assists users in specifying,
	comprehending and maintaining a high level of privacy,

%Privacy Wizard
For example with the Privacy Wizard by Fang and Le Fevre [10].
	the goal of this tool is to automatically configure a user’s privacy settings with minimal effort and interaction from the user.

%PViz
Similarly,
	Mazzia and Adar [20] introduce PViz,
	a system that allows users to understand the visibility of their profiles according to natural sub-groupings of friends,
	and at different levels of granularity.
PViz relies on a graphical output model that illustrates the user’s social network.

%PRISM
Patil and Kobsa [22] propose the PRISM (PRIvacy-Sensitive Messaging) system,
	providing Internet Messaging users with various informative visualizations as well as mechanisms for presenting oneself differently to various groups of contacts.

%risk score
In a similar vein,
	some studies [18, 19] propose a methodology for quantifying the risk posed by a user’s privacy settings.
A risk score reveals to the user how far his/her privacy settings are from those of other users.
It provides feedback regarding the state of his/her existing settings.
However,
	it does not help the user refine his/her settings in order to achieve a more acceptable configuration.

%NARS
On the other hand,
	Carmagnola et al. [9] recently presented So NARS,
	an algorithm for social recommender systems.
So NARS targets users as members of social networks,
	suggesting items that reflect the trend of the network itself,
	based on its structure and on the influence relationships between users’.

%Protect_U
While much work has focused on tools for understanding and adjusting existing privacy settings,
	Protect\_ U uses machine learning techniques to recommend privacy settings based on a user’s personal data and trustworthy friends.
Protect\_ U analyzes user profile contents and ranks them according to four risk levels: Low Risk,
	Medium Risk,
	Risky and Critical.
The system then suggests personalized recommendations to allow users to make their accounts safer.
In order to achieve this,
	it draws upon two protection models: local and community-based.
The first model uses the Facebook user’s personal data in order to suggest recommendations,
The second model seeks the user’s trustworthy friends to encourage them to help improve the safety of their counter part’s account.

%Enabling Collaborative Data Sharing in Google+

Several access control models for OSNs have been proposed recently [19, 20, 24, 25, 30].
The D-FOAF system [30] is primarily a Friend of a Friend (FOAF) ontology-based distributed identity management system for OSNs,
	where relationships are associated with a trust level,
	which indicates the level of friendship between the users participating in a given relationship.

%trust-based access control model
Carminati et al. [19] introduced a conceptually-similar but more comprehensive trust-based access control model,
	which allows the specification of access rules for online resources,
	where authorized users are denoted in terms of the relationship type,
	depth,
	and trust level between users in OSNs.
They also introduced a semi-decentralized discretionary access control model and a related enforcement mechanism for controlled sharing of information in OSNs [20],
	and proposed a semantic web based access control framework for social networks.

%access control model
Fong et al. [25] presented an access control model that formalizes and generalizes the access control mechanism implemented in Facebook,
	admitting arbitrary policy vocabularies that are based on theoretical graph properties.
Gates [21] claimed relationship-based access control as one of new security paradigms that addresses unique requirements of Web 2.0.
Then,
	Fong [24] formulated this paradigm called a Relationship-Based Access Control (ReBAC) model,
		it bases authorization decisions on the relationships between the resource owner and the resource accessor in an OSN.
%collaborative authorization management
However,
	most of these existing work could not model and analyze access control requirements with respect to collaborative authorization management of shared data in OSNs.

%fine-grained access control
Recently,
	semantic web technologies have been used to model and express fine-grained access control policies for OSNs (e.g.,[16, 22, 38]).

Especially,
	Carminati et al. [16] proposed a semantic web based access control framework for social networks.
Three types of policies are defined in their framework,
	including authorization policy,
	filtering policy and admin policy,
	which are modeled with the Web Ontology Language (OWL) and the Semantic Web Rule Language (SWRL).
Access control policies regulate how resources can be accessed by the participants,
	filtering policies specify how resources have to be filtered out when a user fetches an OSN page; and admin policies can determine who is authorized to specify policies.
Although they claimed that flexible admin policies are needed to bring a system to a scenario where several access control policies specified by distinct users can be applied to the same resource,
	the lack of formal descriptions and concrete implementation of the proposed approach leaves behind the ambiguities of their solution.

The need of collaborative management for data sharing,
	especially photo sharing,
	in OSNs has been addressed by some recent research [15, 28, 31, 40, 44].

%collective privacy management
Particularly,
	Squicciarini et al. [40] proposed a solution for collective privacy management for photo sharing in OSNs.
This work considered the privacy control of a content that is co-owned by multiple users in an OSN,
	such that each co-owner may separately specify her/his own privacy preference for the shared content.
The Clarke-Tax mechanism was adopted to enable the collective enforcement for shared content.

%Game theory
Game theory was applied to evaluate the scheme.
However,
	it could be very hard for ordinary OSN users to comprehend the Clarke-Tax mechanism and specify appropriate bid values for auctions.
Also,
	the auction process adopted in this approach indicates only the winning bids could determine who was able to access the data,
	instead of accommodating all stakeholders’ privacy preferences.

%collaborative access control
In contrast,
	we propose a formal model to support the multiparty access control for OSNs,
	along with a policy specification scheme and a simple but flexible conflict resolution mechanism to particularly enable collaborative data sharing in Google+.

%measure privacy risk
Several recent work also explored how to measure privacy risk in OSNs [14, 34, 43].
PrivAwarems
Becker et al. [14] presented PrivAware,
	a tool to detect and report unintended information loss through quantifying privacy risk associated with friend relationship in OSNs.
%privacy score
Liu et al. [34] introduced a framework to compute the privacy score of a user,
	indicating the user’s potential risk caused by her/his participation in OSNs.
Those prior solutions only focused on the privacy settings of users with respect to their profile items.

SHowever,
	our approach measures the privacy risk caused by different privacy concerns from multiple users,
	focusing on content sharing in Google+.

%metadata
A very preliminary analysis of Google+ privacy has been discussed in [37].
They only addressed concern that Google+ shares the metadata of the photos uploaded by its users,
	which could lead to privacy violations,
	and also showed that Google+ encourages their users to provide other names,
	such as maiden names,
	which may help in identity theft.

%E-Mail Prioritization using Online Social Network Profile Distance

%concept of trust
Due to the increased use of OSNs,
	there is a growing number of studies that focus on using social network data for prioritization (scoring) of messages in order to filter unwanted messages in E-mail systems.
The difference between each approach has to do with the way the concept of trust is represented,
	computed and used.
Moreover,
	different ideas are used to apply social informatics to E-mail systems.

%TrustMail
In TrustMail,
	which is a prototype E-mail client,
	an approach is proposed that makes use of OSN reputation ratings to attribute different scores to E-mails [Golbeck and Hendler (2004)].
The actual benefit of this system is that,
	by using social network data,
	it identifies potentially important and relevant messages even if the recipient does not know the sender [Golbeck and Hendler (2004)].

%Reliable Email
The approach of combining whitelists and social networks is proposed in Reliable Email (RE) [Garriss et al. (2006)].
RE exploits social relationships between E-mail senders and recipients to accept potentially relevant and important messages by automatically broadening the users whitelist.

%Ostra
Ostra utilizes the existing trust relationship among users to charge the senders of unwanted messages and thus block spam [Mislove et al. (2008)].

%SoEmail
SoEmail considers the trust as an integral part of networking rather than working alongside of an existing communication system [Tran et al. (2010)].
SoEmail leverages social network data to rate the messages.
The key feature of SoEmail is that instead of directly connecting the sender and the recipient,
	messages are routed through existing friendship links.

%direct experiences
In one proposed model [Yu and Singh (2001)],
	the trustworthiness of a user is calculated based on direct experience with that user as well as the belief rating of her neighbors.

%previous direct interaction
At the same time,
	another team of researchers suggested a model where trust is measured from the performance of previous direct interaction with a user
		as well as the direct interaction and asking trusted users to recommend other users [Esfandiari and Chandrasekharan (2001)].

%Social interactions
Social interactions (e.g.,
	the exchange of messages between users) have been suggested as an indicator of interpersonal tie strength [Xiang et al. (2010)].
As a consequence,
	an unsupervised model has been developed to estimate the relationship strength from the interaction activity and the user similarity in the OSN [Xiang et al. (2010)].

%********
Although all of the aforementioned approaches leverage social relationships for extracting trust,
	the applications are not designed to be automated in the sense that the user must explicitly score other users,
	score messages,
	create whitelists or adjust the credits.
Furthermore,
	none of the aforementioned trust models are utilized to prioritize the E-mail senders.

%**********
Our method lets the user provide feedback about the ranking of E-mail senders and we then employ an automatic means for estimating weights that allows a more personalized ranking of E-mails.
A potential advantage with the latter approach is that users precisely rate senders via rating E-mails without needing a mechanism to map the answers to the user preferences.
Keeping in mind that the prioritization process of E-mails does not depend on the content of messages rather it based on senders and their level of trust with recipients.

%Social network analysis for cluster-based IP spam reputation

%Reputation-based systems.

2.1 Reputation-based systems.
A reputation system collects,
	distributes and aggregates feedback about participants’ past behavior.
Such systems help people decide whom to trust,
	encourage trustworthy behavior and deter participation by those who are unskilled or dishonest (Resnick et al., 2000).
Thus,
	electronic reputation is becoming as valuable an asset as traditional reputation.

%Real-time reputation-based systems.

Various applications use real-time reputation-based systems,
	including online markets and anti-spam solutions (Carrara and Hogben, 2007).
Anti-spam reputation systems generate a score,
	or rating,
	for each incoming message or IP,
	based on analysis of various parameters: e-mail volume,
	type of traffic (e.g. sporadic vs continuous),
	rate of user complaint reports,
	feedback from spam traps,
	compliance with regulations,
	etc.
This aggregated information,
	collected over time,
	forms the reputation of the sender.
A low reputation rating (score) can result in tagging of an IP as spam (Carrara and Hogben, 2007).

%Reputation-based classifying systems using IP clusters.

2.2 Reputation-based classifying systems using IP clusters.
Although spammers can evade list-based filtering using ephemeral IP addresses,
	they have far less flexibility when it comes to altering the network-level properties of the spam they send (Ramachandran and Feamster, 2006).
Recent studies have proposed to enhance the performance of IP reputation-based systems by exploiting information generated from IP clusters,
	which are one of the most substantial network-level features.

%
Krishnamurthy and Wang (2000) defined a cluster as a partitioning of a set of IP addresses into non-overlapping groups,
	where all the IP addresses in a group are topologically close and under common administrative control.
IPs within the same cluster are likely to be subject to similar security or network policies.
IP clusters have been constructed using information from Border Gateway Protocol (BGP),
	autonomous system (AS) number,
	and a combination of BGP and Domain Name Server (DNS) information,
	as described below.

Venkataraman et al. (2007) examined a large corpus of messages,
	comprising of 700 mailboxes and collected over a six-month period.
They show that most spamming IPs are ephemeral (less than ten days),
	as opposed to legitimate IP addresses which were mostly long-lived (at least ten to 30 days).
They also examined the spam ratio of BGP-based network-aware clusters,
	and found that IP addresses responsible for the bulk of spam are well-clustered,
	and that these clusters are long-lived.
The authors concluded that the collective history of IPs as a cluster can be useful in assigning reputations to other IP addresses originating from the same cluster.
Their system for prioritizing legitimate mail uses both the history of individual IP addresses and network-aware clusters to assign reputations to unknown IP addresses.
They demonstrated that the history and structure of IP addresses can reduce the adverse impact of mail server overload by a factor of 3,
	through increasing the number of legitimate e-mails accepted.

However,
	such a reputation-based mechanism is primarily useful for prioritizing legitimate mail,
	rather than actively rejecting suspected spammers.
Therefore,
	this system can be used in combination with other techniques in a general-purpose spam mitigation system (Venkataraman et al., 2007).

IP clusters constructed from AS numbers are highly useful for indicating malicious hosts.
ASes have a stronger association with the sender’s identity than the individual IP address.
This is because the spamming mail server can be constructed within specific ASes without being blocked by the network administration.
In addition,
	bots tend to aggregate within ASes,
	since machines in the same AS are likely to have the same vulnerability.
It is difficult for spammers to transfer the mail servers or bot armies to a different AS (Hao et al., 2009).

%SNARE
A recently developed sender reputation engine named SNARE (Spatio-Temporal Network Level Automatic Reputation Engine),
	can automatically classify e-mail based on a combination of various lightweight network-level features 
	(e.g. geodesic distance between sender and recipient, number of recipients).
The most influential feature in the system was the AS number of the sender.
The system achieved accuracy comparable to existing blacklists:
70 percent detection rate for less than a 0.3 percent false-positive rate (Hao et al., 2009).

%cluster-based reputation
Qian et al. addressed this issue by presenting a clustering technique that refines AS-based and BGP prefix-based clusters.
The authors combined BGP and DNS information to identify a cluster of IP addresses within the same administrative boundary,
	and thus constructed the reputation for an entire cluster.
This cluster-based reputation system allowed more accurate identification of the reputation of previously unknown IP addresses,
	and reduced the false negative rate by 50 percent compared to blacklists,
	without increasing the false-positive rate (Qian et al., 2010).

Systems using whole-cluster information contribute an important solution to the drawbacks of individual IP-based filters.
However,
	cluster-based systems have not yet reached their full potential as accurate and effective determinants of IP reputation.
This is because the clusters need to be sufficiently homogeneous in terms of their legitimate mail/spam behavior,
	so that the bulk of spam can be filtered from the bulk of legitimate mail (Venkataraman et al., 2007).
Yet this is a challenging requirement,
	due to the huge amounts of IPs that clusters generally contain.
Moreover,
	low-grained reputation systems may harm others as well as the attacker – if the “bad” behavior of one sender is taken into account for the whole domain or provider,
	then the other senders associated with that provider will also experience loss of reputation (Carrara and Hogben, 2007).
Therefore,
	existing cluster-based approaches need to rely on additional information such as various spatio-temporal features (Hao et al., 2009),
	or on the less reliable history of short-lived individual IPs (Venkataraman et al., 2007).

%social network analysis metrics
Herein,
	we suggest that the accuracy of cluster-based systems can be improved by using social network analysis metrics.
Using one or more of these parameters may provide an amplified view of the AS by increasing granularity,
	i.e. the sub-division of cluster information.

%Stalking Online: on User Privacy in Social Networks

%*****
In online social networks,
	users are sometimes either oblivious about their privacy,
	or concerned but underestimate the privacy risks.

%Surveys
Surveys and general discussions on social network privacy and security could be found at [21, 5, 41].

In this paper,
	we are interested in the nature of user identity and personal information that are voluntarily released to social networks,
	and how such information could be valuable to attackers.

Along this thrust,
	four types of privacy threats have been discovered in the literature: 
	(1) an individual information item (e.g. an identifiable profile image) may be accessed by adversaries;
	(2) information items of the same user may be collected from different sources,
		and aggregated to reveal user privacy; 
	(3) values of hidden information items may be inferred from public information items;
	and (4) user identities could be recovered from anonymized data sets.
We briefly cover them below.

%Private information disclosure

2.1 Private information disclosure
Users give out information to trusted social network community.
They also implicitly assume that their information would stay within the community.
Unfortunately,
	this assumption is not always valid.
For instance,
	messages sent to an email-based social network may be archived at a repository and accessible to the open public [11],
	stalkers may follow people through social networks [9],
	gadgets and add-ons may access users’ profiles [20],
	code errors reveal user profiles [3],
	etc.

%Privacy score
In [29, 30],
	authors proposed a framework that assesses potential privacy risks to a privacy score,
	which is computed from the sensitivity of the disclosed information and the visibility of such information.

%third parties
Finally,
	[23] shows that online social networks and applications leak users’ personally identifiable information to third parties.

%functions to discover hidden social relationships
In a position paper [37],
	the author identifies an attack that uses Sybil nodes and search functions to discover hidden social relationships in LinkedIn.

%Information aggregation
2.2 Information aggregation
When people participate in online social networks,
	they voluntarily release different types of personal information: name,
	screen name,
	telephone numbers,
	email addresses,
	locations,
	etc.
Moreover,
	when users post messages in forums,
	blogs,
	and bulletin boards,
	they also disclose small pieces of private information.
However,
	with the development of information retrieval techniques,
	adversaries could collect pieces of such personal information of the targeted user [32].
Though a single piece of such information may be harmless,
	it discloses a significant amount of private information when associated with other pieces of information.

%Inference attacks
2.3 Inference attacks
Aside from voluntary disclosure of explicit personal information,
	[17, 16, 43] study a type of indirect private information inference through social relations.
	[17, 16] notice that hidden attributes could be inferred from friends’ attributes using a Bayesian network.
They study the factors that impact inference accuracy,
	and suggest that selectively hiding social connections or friends’ attributes could help preserve privacy.

%Privacy Threats in Published Social Network Data

2.4 Privacy Threats in Published Social Network Data
When social network data sets are published for various legitimate reasons,
	user identity and some profile information are often removed to protect the user privacy.
Some of the well-known techniques for this purpose includes k-anonymity [39, 1],
	l-diversity [33] and t-closeness [26].
For instance,
	in a k-anonymized data set,
	an individual cannot be distinguished by attributes from other k-1 records.
Possibilities of attribute re-identification attacks on publicly available data sets have been studied in [38, 13, 25].

More recently,
	in [7] authors introduce an attribute-based anonymization method for social network data.
On the other hand,
	due to the nature of social network data,
	just anonymizing node attributes is not enough.
Graph structure contains significant amount of information which could be utilized to hurt user privacy,
	i.e. structural reidentification attacks.

A good survey on structural anonymization and re-identification attacks could be found at [45].

Notably,
	[2] first identified the problem that although quasiidentifiers are removed before publishing,
	node identities could be inferred through graph structure.
They show that node identities are vulnerable to both passive and active attacks.

In [35],
	authors introduce a new metric,
	namely topological anonymity,
	to quantify the level of anonymity using the topology properties of network graph. [44] introduces neighborhood attacks,
	in which an adversary knows the neighborhood subgraph of the target,
	and tries to reidentify the user from an anonymized network graph.
They propose an approach to further anonymize vertexes by modifying edges to construct isomorphic neighborhoods.

In [28],
	authors define k-degree anonymity: in a k -degree anonymized graph,
	each node has the same degree with at least k other nodes.
They also efficiently propose k -degree anonymize graphs with minimal edge additions and deletions.

Moreover,
	[15] models three types of adversary knowledge that could be used to re-identify vertexes from an anonymized social network graph.
They tackle the problem through graph generalization – dividing the graph into partitions and publishing summarized partition-level data.
K-Automorphism is introduced in [46] to defend against multiple attacks.

In [18],
	authors propose a graph anonymization approach that maximally preserves original graph structure and statistical features.

Finally,
	[31] considers social network as a weighted graph,
	in which edge labels are also considered to be sensitive.
They propose to protect sensitive edge labels while keep certain global features of the graph.

%Interdependent Privacy: Let Me Share Your Data

%permission systems
A number of other works (e.g.,
	[14–17]) have commented on the weaknesses of app permission systems in safeguarding user privacy and presented ways for improvement.
In particular,
	Wang et al. [17] presented some insights regarding app permission dialogues,
	and gave an example where installing a given calendar app would violate the user’s global privacy setting.

%infer private user information
Researchers [18, 19] demonstrated the ability to infer private user information using only friendship links,
	group memberships and information shared by others publicly.

%collaborative
Dealing with explicit collaborative information sharing,
	Hu et al. [20] proposed a method to detect and resolve privacy conflicts.

Here,
	we focus on the interdependent nature of app privacy: we study how the Facebook permission system affects not only the user installing an app but also his friends.

%What Matters to Users? Factors that Affect Users’ Willingness to Share Information with Online Advertisers

%third-party
In OBA,
	third-party advertisers track users as they browse websites.
The purpose of this tracking is to build profiles of users in order to target ads.
Tracking can be performed using third-party cookies or more complex techniques [19].

Third-party tracking for OBA is widespread.
In 2011,
	third-party trackers were present on 79\% of pages examined among the Alexa top 500 websites [27].
Among a set of selected U.S. and Canadian health websites, 85\% contained at least one tracker [3].
In a study performed from 2005 to 2008,
	Krishnamurthy and Wills found the number of thirdparty trackers was growing,
	while the number of companies controlling the collected information was shrinking [14].

%footprint
This large data footprint leads to privacy concerns.
Retailers can combine credit or debit card histories with data from online tracking to create detailed customer profiles revealing potentially sensitive “lifestyle or medical issue[s]” [9].

Even when data is collected in an aggregated,
	ostensibly anonymized manner,
	bulk collection leaves the potential for re-identification [7, 24].

Users tend to dislike the idea of being tracked and profiled by third parties [8].
Based on 48 semi-structured interviews,
	Ur et al. found that participants recognized both pros and cons of OBA,
	pointing out that OBA can be useful to both users and companies,
	yet also privacy-invasive.
Participants did not understand how ads were targeted and believed companies collected more information than they generally collect [34].
Respondents to a 2011 survey conducted by McDonald and Peha also believed that websites could collect more data than is currently accessible [21].
Wills and Zelijkovic created a JavaScript tool to show a user’s location,
	age range and gender,
	all of which could be determined by third-party sites.
Approximately half of their 1,800 participants were concerned about third-party tracking,
	the level of data collection,
	and the ability of third-party data trackers to infer demographic information after seeing these data [38].

OBA is currently self-regulated under guidelines created by the advertising industry.
These guidelines require that users be provided the option to opt out of targeted ads [25],
	but do not mandate options for fine-grained control [19].
Under current guidelines,
	users are notified of the presence of behavioral advertising through an AdChoices icon that appears near or inside the ad.
However,
	Leon et al. found that most users did not understand the purpose of this icon [17].

Users are currently able to limit online behavioral advertising in several ways.
By clicking on the AdChoices icon,
	users can visit an opt-out page that allows them to opt out of ad networks individually.
Alternatively,
	they can use thirdparty tools that block tracking,
	or even block ads entirely.

However,
	there are problems with all of these opt-out mechanisms.
For instance,
	Komanduri et al. found many gaps in advertising networks’ compliance with industry opt-out requirements [12],
	while Leon et al. found serious usability issues with all nine privacy tools they tested,
	including tools provided by industry coalitions and by third parties [16].

Users could instead follow links to natural-language privacy policies and evaluate hundreds of companies,
	but the opportunity cost of doing so would be prohibitive [20].

Another option for opting out of OBA is the “Do Not Track” initiative.
In most modern web browsers,
	users can set a “Do Not Track” (DNT) signal to be sent to third-party advertising servers.
This option is generally easy for users to set,
	though it lacks the fine-grained control needed to set preferences on a per-advertiser basis [31].
However,
	there is not yet a general consensus on the meaning of DNT or how advertisers should respond to the DNT signal.
Further,
	users do not have a good idea of what the DNT button in their browser will actually do [21].
Despite these issues, 12 percent of desktop users and 14 percent of Android users have enabled DNT in Firefox [30].

There have been indications that users may be more comfortable sharing their data if provided with more granular choice or better control over their data sharing.
Gomez et al. found that a large percentage of complaints submitted to the FTC from 2004 through 2008 were related to a lack of user control [8].
Users’ willingness to share data tends to depend on context and the type of data being requested.

Broadly,
	reputation and brand name have been found to influence user trust of websites [4].

Although factors influencing OBA decision-making have not been well studied,
	a handful of researchers have examined factors that impact information sharing more broadly.

In a 1998 study of 401 participants,
	Awad and Krishnan found that users who valued information transparency were more concerned about being profiled online than those who did not [2].
Across two studies,
	Acquisti et al. found that the context of an information request affected users’ willingness to share information.
If more sensitive information was requested prior to less sensitive information,
	participants were more likely to reveal more information overall [1].
Taylor et al. found that general online trust made users less concerned about privacy [33].
Joinson et al. asked distance-learning students to sign up for a panel that requested a variety of sensitive personal information.
They found that participants were least willing to share financial information [10].

Two studies have examined information sharing with online music sites.
When asked to provide information to a mock online music retailer,
	Metzger found that participants were more likely to disclose information if they saw a strong privacy policy than if they saw a weak privacy policy.
She also found that participants were most likely to be willing to provide information necessary for a retail transaction,
	specifically name and address,
	as well as basic demographic information.
Participants were least likely to be willing to provide financial information [23].
In a study of how participants felt about revealing information to a music recommender system,
	van de Garde-Perik et al. found that some participants wanted to reveal information anonymously because of privacy concerns,
	while other participants were willing to reveal information tied to their identities to help improve the system.
In both cases,
	the researchers found that participants wanted to know how the data would be used and who would have access to it [35].

In this paper,
	we examine which types of data users are more willing or less willing to allow websites to collect for OBA purposes based on different factors,
	including familiarity with the site,
	the length of time for which data will be retained,
	and the scope in which that data will be used.

%Leveraging Social Networks to Combat Collusion in Reputation Systems for Peer-to-Peer Networks

%Reputation systems.
In recent years,
	many reputation systems have been proposed.
%	many reputation systems [10],[12],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27] have been proposed.

%three basic trust parameters and two adaptive factors
PeerTrust [18] computes peer reputation scores based on three basic trust parameters and two adaptive factors.

%anonymity
Trustme [19] offers an approach toward anonymous trust management,
	which can provide mutual anonymity for both the trust host and the trust querying peer.

%Reputation hash table
EigenTrust [10] and PowerTrust [20] depend on the distributed hash tables to collect reputation ratings and calculate the global reputation value of each peer.

%behavioral
TrustGuard [12] incorporates historical reputations and behavioral fluctuations of nodes into the estimation of their trustworthiness.

%fuzzy logic inferences
FuzzyTrust [21] uses fuzzy logic inferences to better handle uncertainty,
	fuzziness,
	and incomplete information in peer trust reports.
	
%weighted local trust scores
GossipTrust [22] enables peers to share weighted local trust scores with randomly selected neighbors until reaching global consensus on peer reputations.

%score for peers
Cornelli et al. [25] proposed an approach to P2P security that enables each client to compute a personalized,
	rather than global,
	performance score for peers,
	and also distinguish peer performance from peer credibility.

%peer voting
Both XRep [26] and X2 Rep [27] extend the work in [25] by additionally computing object reputations based on weighted peer voting.

EigenTrust [10] breaks collusion collectives by assigning higher weight to the feedback of pre-trusted peers.

%reputation values *********
Moreton and Twigg [28] proposed the Stamp algorithm,
	where peers issue stamps as virtual currency for each interaction,
	and the value of each peer’s stamps is maintained by exchange rates that act as reputation values.

%traffic logs
Lian et al. [7] analyzed the traffic logs in a P2P file sharing system to study different types of collusion patterns.

%experience
The works in [13],[14],[15] let a peer evaluate others’ trustworthiness based on its experience.
Sorcery [16] lets clients utilize the overlapping voting histories of both their friends and the content providers to judge whether a content provider is a colluder.

Our proposed method is the first that leverages social distance and interest relationship from a social network to identify suspicious collusion and to reduce its influence on node reputation.

%SONET: A SOcial NETwork Model for Privacy Monitoring and Ranking

Social media privacy has raised many concerns and may affect individuals,
	enterprises,
	legislatures,
	and government agencies.
Different types of attacks have been investigated in [2][3][4][5] and a few privacy preserving schemes are proposed in [6][7].

%privacy preserving & privacy policy & privacy measurement
Previous works on social media privacy focus on privacy preserving [8][9] and privacy policy conflicts [9][10].
Few works [12][13][14] [15] have been conducted on privacy measurement due to the challenges to quantify the privacy risk associated with online social network users.

%Quantify the privacy risks
Recent works attempting to quantify the privacy risks associated with the usage of online social networks can be found in [12][13][14] [15].
In [12],
	the authors propose to use the amount of information revealed in online social networks to quantify the privacy risks.

%Privacy score
In [13],
	the authors present an approach in which privacy score is calculated by computing sensitivity and visibility of attributes. 
A Naive approach for evaluating sensitivity and visibility of attributes is demonstrated in [13].
The authors extend their works to another approach in [14].
They use Item Response Theory (IRT) to evaluate sensitivity and visibility of attributes when evaluating privacy scores. .

%Privometer
The authors in [15] develop a tool,
	Privometer,
	to meas sure information leakage.
The leakage is indicated by a numerical value.
The tool can suggest self sanitization actions based on the numerical value.

%Privacy as a Product: A Case Study in the m-Health Sector

The study of privacy measurement applied to ICT is very wide and embraces many fields of knowledge from Cryptography and Statistics to Artificial Intelligence and Sociology.

When an ICT user accesses a service,
	in general,
	she has to share some information with the provider of the service,
	namely identity,
	type of service required,
	location,
	etc.
Clearly,
	the shared information depends on the service but regardless of the exchanged information,
	users have to choose between four main options: 
	(i) trust the provider and send him all the required information,
	(ii) individually protect the data sent to the untrusted provider,
	(iii) collaborate with the provider to protect her data or,
	(iv) collaborate with other users to protect their data from the provider.

In the following sections we briefly summarize the most relevant techniques within each category.

A.
%Privacy based on trust 
Privacy based on trust:
This is probably the most common situation.
Users tend to trust service providers because,
	they do not really have alternatives in many cases.
Due to the fact that privacy is considered a right,
	most countries have regulations that compel companies and agencies to guarantee the privacy of their users.
In addition to this legislation,
	companies might decide to adhere to a privacy policy that describes their practices according to the privacy of their users.
Finally,
	when users data are released to third parties they should be sanitized so as to guarantee users privacy.
To do so statistical disclosure control techniques are generally used.

B.
%Privacy based on Individual 
Privacy based on Individual:
User Actions Despite the legislation,
	users might prefer to keep some of their private information away from the service provider.
In this case,
	we assume that the user cannot collaborate with the service provider (e.g. consider the case of sending a query to an Internet search engine such as Google or Yahoo.
The user cannot initiate a collaborative protocol with the search engine,
	that is only able to receive and answer queries).

C.
%Privacy based on Collaboration with the Provider
Privacy based on Collaboration with the Provider:
There are situations in which the service provider might collaborate with the user to protect her privacy by running privacy-aware protocols.
We emphasize the following: 
	• Privacy Preserving Data Mining 
	• Private Information Retrieval.

D.
%Privacy based on Collaboration with other Users
Privacy based on Collaboration with other Users:
This is an evolution of the proposals described in II-B in which users collaborate to protect their privacy.
In this case,
	users do not want to trust the provider nor other third parties.

We emphasize the following approaches:
	• Distributed obfuscation [14][12].
	• Distributed pseudonymizers [10].

%Mean privacy: A metric for security of computer systems

The challenging area of quantitative security evaluation has been received much more attention in recent years.
In this section,
	we want to provide a general overview of the most important methods worked out in this important area of security analysis.

%Probabilistic and stochastic models

Due to the incomplete administrator’s knowledge of the behavior,
	intent and skill level of attackers,
	the prediction of their behavior would be a very difficult task.
That is,
	due to the uncertainty in the attack process,
	it cannot be completely captured.
Thus,
	the use of probabilistic and stochastic models and concepts in the area of security evaluation can be an appropriate approach.
As will be shown,
	stochastic and probabilistic modeling techniques used in the context of dependability evaluation,
	have been extended and also used to evaluate certain security metrics.
In a number of publications,
	state-based stochastic models,
	like Markovian models or stochastic Petri nets,
	have been introduced as useful tools for quantitative security analysis.
On the other hand,
	probabilistic models have also extensively been used to evaluate security metrics.
For instance,
	some models such as attack graphs,
	Bayesian networks and model checking can probabilistically be defined and analyzed.

%parameterized with continuous probability distributions
In contrast to stochastic models,
	which are parameterized with continuous probability distributions,
	these types of models are primarily parameterized with discrete probability distributions.

%Game theory
Sallhammar et al. [1] suggest the use of game theory as a method for computing the probabilities of expected attacker behavior in a quantitative stochastic model of security.
By viewing system states as elements in a stochastic game,
	they compute the probabilities of expected attacker behavior and model attacks as transitions between the system states.
Having solved the game,
	the expected attacker behavior is reflected in the transitions between the states in the system model,
	by weighting the transition rates according to probability distributions.
The proposed game model is based on a reward and cost concept and a detailed evaluation of how the reward and cost parameter influence the expected attacker behavior is included.
In the final step,
	continuous-time Markov chain (CTMC) is used to compute operational metrics of the system.

%
In [2],
	Madan et al. used the state transition model of the scalable intrusion-tolerant architecture (SITAR) proposed in [3] and stochastic modeling techniques to capture the attacker behavior.
The security quantification analysis is first carried out for steady-state behavior leading to metrics like steady-state availability.
By transforming this model into a model with absorbing states,
	they computed a security metric called the mean time to security failure (MTTSF) and also the probabilities of security failure due to violations of different security attributes.
%
Wang et al. [4] developed a stochastic reward net (SRN) model to capture attacker behavior as well as system response for the intrusion-tolerant system named SITAR.
It was shown that the resulting analysis is useful in determining gains in security by reconfiguring such a system in terms of increase in redundancy under varying threat levels.

%
Stevens et al. [5] described a probabilistic model for validating an intrusion-tolerant system that combines intrusion tolerance and security.
The models were built with stochastic activity networks (SANs) formalism using the Möbius tool.
This paper illustrates how probabilistic modeling can be used in an integrated validation procedure and successfully brings insight and feedback to a design.

In [7],
	Kaâniche et al. presented some empirical analyses based on the data collected from the Leurré.com honeypot platforms deployed on the Internet.
They provided some preliminary statistical modeling studies such as the analysis of the time evolution of the number of attacks taking into account 
the geographic location of the attacking machine,
	the characterization and statistical modeling of the times between attacks and the analysis of the propagation of attacks throughout the honeypot 
	platforms in order to characterize attack processes.
In [8],
	Jonsson et al. worked out a hypothesis on typical attacker behavior based on empirical data collected from intrusion experiments;
		attacking process can be split into three phases: the learning phase,
	the standard attack phase,
	and the innovative attack phase.
The collected data indicated that the times between breaches during the standard attack phase are exponentially distributed.
Oratalo et al. [9] provided the results of an experiment of security evaluation.
The evaluation is based on a theoretical model called the privilege graph and transformed into a Markov model,
	which describes the system vulnerabilities that may offer opportunities to potential attackers to defeat some security objectives.
They studied several modeling assumptions and discussed the validity of these assumptions based on an experimental study performed on a real system during more than a year.

McQueen et al. [10] proposed a new model for estimating the time to compromise of a system component that is visible to an attacker.
The model provides an estimate of the expected value of the time-to-compromise as a function of known and visible vulnerabilities,
	and attacker skill level.
The time-to-compromise random process model is a composite of three subprocesses associated with attacker actions aimed at the exploitation of vulnerabilities.
As another example,
	Leversage et al. [11] proposed a mean time-to-compromise interval as an estimate of the time it would take for an attacker,
		with a specific skill level, 
		to successfully impact a target system and a state space model and algorithms for estimating attack paths and state times to calculate these intervals for a given target system.
For estimating state times,
	they have used a statistical algorithm based on a modified variant of McQueen’ time-to-compromise model.
Furthermore,
	Paulauskas et al. [12] proposed a method for security evaluation by using mean time-to-compromise criteria.
They postulated that normal distribution should be used for mean time-to-compromise evaluation in the attacker skill level group instead of top attacker skill values,
	because the beginners initiate a greater number of attacks.

Xu et al. [13] proposed a stochastic model for quantifying security of networked systems.
For this purpose,
	a vulnerability graph is used to abstract a networked system and a stochastic process used to describe attacks over the vulnerability graph.
They investigate the problem from a high-level abstraction,
	which also leads to both analytical results and practical methods for obtaining the desired security quantities.
Basagiannis et al. [14] introduced probabilistic model checking as an efficient tool-assisted approach for systematically quantifying denial of service (DoS) security threats.

In order to do so,
	a security protocol with a fixed network topology using probabilistic specifications is modeled
		,a probabilistic attacker model which performs DoSrelated actions with assigned cost values attached into the protocol model.
From the developed model,
	a discrete-time Markov chain (DTMC) via property preserving discrete-time semantics is obtained.
The DTMC model is verified using the PRISM model checker that produces probabilistic estimates for the analyzed DoS threat.

In [15],
	Bodei et al. have used a special process algebra for predicting quantitative metrics such as the costs of successful attacks on systems describing cryptographic protocols.
They have described protocols and their attacks using the process algebra and have associated a cost with each transition in the transition system.
The transition system is then mapped to a CTMC and the performance of system is evaluated.
Ahmed et al. [16] proposed a novel security metric framework that identifies and quantifies objectively the most significant security risk factors,
	which include existing vulnerabilities,
	historical trend of vulnerability of the remotely accessible services,
	prediction of potential vulnerabilities for any general network service and their estimated severity and finally policy resistance to attack propagation within the network.

Using real life vulnerability data of the past six years from national vulnerability database (NVD),
	they validated this hypothesis that if a service has a highly vulnerability-prone history,
	then there is higher probability that the service will become vulnerable again in near future.
In [17],
	Cervesato has investigated intruder models that rely on capabilities beyond Dolev-Yao gentlemen correctness.

This quantitative approach enables evaluating protocol resilience to various forms of DoS,
	guessing attacks and resource limitation.

While the methodology is general,
	it is demonstrated through a low-level variant of the multi-set rewriting (MSR) crypto-protocol specification language.

Hecker et al. [18] present two principally possible and distinct approaches to an operational security assurance evaluation of networked IT systems and discuss their pros,
	cons and limits,
	illustrated through examples.
Their analysis clearly distinguishes security assurance from related subjects like security,
	dependability and trust.
In [19],
	Jafari et al. propose an approach for developing security metrics to be used for assessing security posture of healthcare organizations.
The metrics for this approach shall not be tailored to any specific organization to ensure comparable results.
The approach is geared to articulate and visualize healthcare specific security concerns to enable stakeholders in this field to grasp an understanding of the security of their systems.
The aim is to foster confidence in sharing sensitive patients’ information.

%graph-based probabilistic metri
Wang et al. [20] propose an attack graph-based probabilistic metric for network security and study its efficient computation.
They first define the basic metric and provide an intuitive and meaningful interpretation to the metric.
They then study the definition in more complex attack graphs with cycles and extend the definition accordingly.
They show that computing the metric directly from its definition is not efficient in many cases and propose heuristics to improve the efficiency of such computation.

%technical security metrics*********
Boyer et al. [21] proposed a specific set of technical security metrics for use by the operators of control systems.
Their proposed metrics are based on seven security ideals associated with seven corresponding abstract dimensions of security.
They have defined at least one metric for each of the seven ideals.
Each metric is a measure of how nearly the associated ideal has been achieved.
These seven ideals provide a useful structure for further metrics development.
A case study shows how the proposed metrics can be applied to an operational control system.

%Four security metrics
Lippmann et al. [23] have introduced meaningful security metrics that motivate effective improvements in network security.
They present a methodology for directly deriving security metrics from realistic mathematical models of adversarial behaviors.
Four security metrics are described that assess the risk from prevalent network threats.
These initial four metrics and additional ones should be added incrementally to a network to gradually improve overall security as scores drop.

%Markov chains
Finally,
	Moayedi et al.[25] introduce a novel approach to extend the basic ideas of applying game theory in stochastic modeling.
The proposed method classifies the community of hackers based on two main criteria used widely in hacker classifications,
	which are motivation and skill.
They use Markov chains to model the system and compute the transition rates between the states based on the preferences and the skill distributions of hacker classes.
The resulting Markov chains can be solved to obtain the desired security measures.

%Online Privacy as a Collective Phenomenon

Understanding privacy in OSNs starts with understanding the individual risk of sharing information with undesired contacts [6, 24].
Recent technologies promise to alleviate user privacy concerns.
For example,
	distributed recommender systems can put a limit to privacy disclosure [21],
	deployment of OSNs in the cloud can avoid the centralization of user data [44, 46],
	techniques for picture encryption [43] and content anonymization [38] can prevent undesired access to private content.

%Ethical issues
Private information about users can be a source of wealth,
	e.g. by significantly increasing the revenue of personalized advertisement [19].
A possible solution for this dilemma is to create monetization schemes that allow users to set up the price they request for companies to access their private information [41],
	effectively creating privacy butlers that automatically control privacy [45, 27].
This approach can be criticized for its ethics about the value of privacy,
	asking if market dynamics would push less wealthy individuals to have no privacy [36].

Even with full individual control,
	the possibility of third-parties to infer private attributes still exists [35].

%Link prediction problem
The link prediction problem has also been applied to predict links between non-users of Facebook [20],
	given only the link information towards non-members from the known network.
Additionally,
	the network completion problem aims to infer both missing links and nodes,
	where it has been shown that the missing part of the network can be inferred based only on the connectivity patterns of the observed part [5].

%Inference problems
Previous studies of user privacy have focused on sensitive attribute inference problems,
	where user private attributes are detected based on a mix of public profiles in the network,
	friendship links and group membership information of private users [47].
Specifically,
	within the friendship identification and inference attack [23],
	a user might aim to infer private attributes of another user.

%Leveraging Social Networks for Effective Spam Filtering

%Identity-based spam filtering approaches

%whitelist & blacklist
The simplest identity-based spam filtering approaches are blacklist and whitelist [19]–[22],
	which check the email senders for spam detection.
Whitelists and blacklists both maintain a list of addresses of people whose emails should not and should be blocked by the spam filter, respectively.

%Blacklist
One server side solution [22] records the number and frequency of the same email sent to multiple destinations from specific IP addresses.
If the number and frequency exceed thresholds,
	the node with the specific IP address is blocked.

%clustering coefficient
Boykin et al. [3],[25] constructed a graph in which vertices represent email addresses and direct edges represent email interactions.
Emails are identified as spam,
	valid,
	or unknown based on the local clustering coefficient of the graph subcomponent.
This is based on the rationale that the social communication network of a normal node has a higher clustering coefficient than that of a spam node.

%RE
RE [23] is a whitelist spam filtering system based on social links.
It is based on the assumption that all friends and FoF are trustable.

%LENS
Hameed [24] proposed LENS,
	which extends the FoF network by adding trusted users from outside of the FoF networks to mitigate spam beyond social circles.
Only emails to a recipient that have been vouched by the trusted nodes can be sent into the network.

%social
DeBarr et al. [31] evaluated the use of social network analysis measures to improve the performance of a content filtering model.
They tried to detect spam by measuring the degree centrality of message relay agents and the average path length between senders and receivers.
They claimed that the messages from a promiscuous mail relay or messages with unusual path lengths that deviate from the average are more likely to be spam.

%Legitimacy scores
Lam et al. [32] proposed a learning approach for spam sender detection based on user interaction features (e.g.,
	indegree/outdegree and interaction frequency) extracted from social networks constructed from email exchange logs.
Legitimacy scores are assigned to senders based on their likelihood of being a legitimate sender.

%SocialEmail
Tran et al. [33] implemented an email client called SocialEmail,
	which provides social context to messages using a social network’s underlying social graph.
This not only gives each email recipient control over who can message him/her,
	but also provides the recipient with an understanding of where the message socially originated from.
However,
	if a spammer compromises a legitimate user’s computer,
	the spammer can easily attack the user’s friends in the social network,
	which is characterized by high clustering and short paths [27].
Also,
	such social interaction-based methods are not sufficiently effective in dealing with legitimate emails from senders outside of the social network of the receiver.
Golbeck et al.

%SOAP
[26] proposed an email scoring mechanism based on an email network augmented with reputation ratings.
An email is considered spam if the reputation score of the email sender is very low.
Different from these social network based methods,
	SOAP focuses on personal interests in conjunction with social relationship closeness for spam detection.

%Identifying hidden social circles for advanced privacy configuration

(Fabeah Adu-Oppong et al., 2008) develop privacy settings based on a concept of social circles which consist of clusters of friends formed by partitioning users friend lists.

%inference of private data from public one
In general,
	semi-supervised learning has been used in social networks to infer users’ private information from the public labeled and unlabeled data using graph based semi-supervised learning,
	e.g. (Javed and Shehab, 2012).

%privacy wizard
For instance,
	Fang et al. (Fang and LeFevre, 2010) proposed a privacy wizard to help users grant privileges to their friends.
The wizard asks users to first assign privacy labels to selected friends,
	and then uses this as input to construct a classifier which classifies friends based on their profiles and automatically assign privacy labels to the unlabeled friends.

%privacy score
Liu and Terzi (2009) have defined a mathematically sound methodology for computing users privacy scores in online social networks.
The privacy score indicates the users potential risk caused by his or her participation in the network.
The authors definition of privacy score satisfies the following intuitive properties: the more sensitive information a user discloses,
	the higher his or her privacy risk.

%grouping friends
Jones et al. (Jones and O’Neill, 2010) investigate users’ rationales for grouping friends,
	for privacy management purposes,
	within online social networks.
They identify six static criteria for grouping,
	and evaluate the similarity of these criteria to the output of standard clustering techniques of users’ friends.
Their work supports our notion that standard clustering techniques can assist users in placing friends into groups analogous with privacy intentions.

%collaborative filtering
Finally,
	collaborative filtering is popular in recommender systems (Su and Khoshgoftaar, 2009),
	and has been recently adopted for policy recommendations (Shehab and Touati, 2012).

%Social Network Privacy Measurement and Simulation

Few works have been conducted on privacy measurement for online social networks.

%PrivAware
In [1],
	the authors propose to use the amount of information that can be inferred from social networks to quantify the privacy risks.
A tool,
	PrivAware,
	is designed to detect and report unintended information disclosures in online social networks.
PrivAware employs inference model which is based on the fact that information about users can be inferred from their social graph.
Privacy score is calculated as total number of attributes visible to the third party applications divided by total number of attributes per participant.
The measured percentage is then mapped to a letter grade,
	where A score represents very few attributes being revealed and F score indicates that privacy risk to the threat of a malicious third party application is high.

%Privacy score
In [2],
	the authors present an approach in which privacy score is calculated by computing sensitivity and visibility of attributes.
Naive approach for evaluating sensitivity and visibility of attributes is demonstrated in [2].
The authors further extend their works to another approach in [3].
They use Item Response Theory (IRT) to evaluate sensitivity and visibility of attributes when calculating privacy score.
The authors use both synthetic and real-world data to show the effectiveness of their approach.

%Privometer
The authors in [4] develop a tool,
	Privometer,
	to measure information leakage based on user profiles and their social graph.
The leakage is indicated by probability.
Privometer is based on an augmented inference model where a potentially malicious application installed in the user’s friend profiles can access substantially more information.
Privometer is implemented as a Facebook application.
It operates in two modes.
In online mode,
	inference is performed based on the friend’s profile where most frequently value is selected.
In offline mode,
	it uses only immediate friends and "network-only Bayes classifier" to measure the probability of inference.

%risk labeling approach
In [6],
	the authors use risk labeling approach to tag users based on the community members’ feedback.
Active learning method is used to correctly label strangers.

In [7],
	the authors conduct a thorough study and analysis of the privacy practices and policies of various online social networking sites.
45 different sites are studied with 260 features to evaluate them.
The work in [7] does not measure the privacy scores of an individual user.
However,
	it presents the concept of the privacy ranking of the websites which can allow the users to make their decisions on the basis of website privacy scores.

%Trust-Aware Privacy Evaluation in Online Social Networks

Researchers studied privacy protection from two directions.

%fundamental changes
Along the first direction,
	fundamental changes to social networking sites were suggested to enhance user privacy.
For example,
	Baden et al. [6] proposed a new type of OSN using encryption to hide user data and allowing user to define privacy policies.

%developing privacy tools
The second direction is developing privacy tools based on existing OSNs.
For example,
	the tradeoff between social network utility and personal privacy was studied in [8].
Fang et al. [5] developed privacy wizards to give user recommendation for privacy setting.
Gundecha et al. [4] proposed an approach to identify a user’s vulnerable friends.

In this paper,
	we propose to assist user privacy protection by providing quantitative evaluation of privacy risk.
Our work belongs to the second category.

There have been several quantification models for privacy evaluation in OSNs.

%individual vulnerability
Alim et al. [9] examined the visibility of OSN users’ profiles and computed the clustering coefficient to compose individual vulnerability.
Based on individual vulnerability,
	relative vulnerability and absolute vulnerability were calculated.

%TAPE
This work approaches the privacy quantification problem from a different angle.
That is,
	to consider how likely a friend reveals others’ personal information,
	described by the privacy trust concept,
	which is a widely studied research problem [11].

%TAPE
The proposed work is also related to information diffusion in OSNs [7].
For example,
	researchers attempt to build mathematical model to solve problems of information diffusion in OSNs,
	such as [12].
Different from the previous work,
	the proposed TAPE framework considers information diffusion in the context of privacy protection,
	which requires different sets of features and considerations.
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Identifying Spam Without Peeking at the Contents

To understand and address the problem,
	we must first establish common definitions,
	explain why the problem exists,
	discuss limitations of current solutions,
	and make suggestions of work that can lead to solutions to alleviate the privacy issues.
In this work,
	we deal with the Internet traffic,
	and in particular with messaging traffic.

%To Join or Not to Join: The Illusion of Privacy in Social Networks with Mixed Public and Private User Profiles
According to Li et. al. [11],
	there are two types of privacy attacks in data:
	identity disclosure and attribute disclosure,
	and identity disclosure often leads to attribute disclosure.
Identity disclosure occurs when the adversary is able to determine the mapping from a record to a specific real-world entity (e.g. an individual).
Attribute disclosure occurs when an adversary is able to determine the value of a user attribute that the user intended to stay private.
We are interested in attribute disclosure in online social networks using the public profiles,
	friendship links and group memberships.

The privacy literature recognizes two types of privacy mechanisms:
	interactive and non-interactive [6].
In the interactive mechanism,
	an adversary poses queries to a data-base and the database provider gives noisy answers.
In the non-interactive setting,
	a data provider releases an anonymized version of the database to meet privacy concerns.
Even though our work is closer to the non-interactive setting,
	the goal of our data provider is not to anonymize a dataset but to ensure that users’ private data remains private and cannot be inferred using links,
	groups and public profiles.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%technical

%\subsubsection{Network layer}

%The first class of prior work focuses on privacy threats in network layer.
%Client and Server IP addresses are the 2 most important fields contained in the Internet Protocol (IP) headers.
%Client IP address is certainly the field carrying more sensible information,
%	it uniquely identifies a particular user’s home gateway.
%The Server IP address,
%	in its turn,
%	contains information about customers enjoying Web services.

%To prevent this information leakage,
%	users usually exploit anonymity network tools (TOR).
%Tor is a software that can be used for anonymization of Internet communication,
%	by routing traffic through several layers of encrypted tunnels.
%However,
%	with the advent of CDNs and Cloud Providers,
%	client IP addresses is mandatory to provide good services.

%The popular technique used to preserve privacy in IP-based network data is IP address anonymization.
%Several algorithms can be exploited to this end,
%	with the most popular being Crypto-PAn [14],
%	a cryptography-based sanitization tool.
%However,
%	IP address anonymization still poses several challenges and questions [15],[16].

%\subsubsection{Transport layer}

%The second class of prior work focuses on privacy threats in Transport layer.
%Transport layer builds on two widespread protocols,
%	namely TCP and UDP.
%Their headers contain poor personal information fields,
%	the only interesting fields are the port number.
%The server port number can give a hint about the network service contacted,
%	as the list of well known ports (0-1023) include popular services such as HTTP, TLS, or DNS.

%An Effective Defense Against Email Spam Laundering
%Anti-Spam techniques

%Many anti-spam techniques have been proposed and deployed to counter email Spam from different perspectives.
%Based on the placement of anti-Spam mechanisms,
%	these techniques can be divided into two categories: recipient-oriented and sender-oriented.

%\subsubsection*{Recipient-oriented Techniques}

%This class of techniques either
%	(1) block/delay email spam from reaching the recipient’s mailbox or
%	(2) remove/mark Spam in the recipient’s mailbox.
%Due to the flourish of techniques in this category,
%	we further divide them into content-based and non-content-based sub-categories.

%\subsubsection{Content-based Techniques}

%The techniques in this sub-category detect and filter Spam by analyzing the content of received messages,
%	including both message header and message body.

%Email address filters: Email address filters are simply whitelists or blacklists.
%Whitelists consist of all acceptable email addresses and blacklists are the opposite.
%Blacklists can be easily broken when spammers forge new email ad- dresses,
%	but using whitelists alone makes the world enclosed.
%Garriss et al. [18] developed a new whitelisting system,
%	which can automatically populate whitelists by exploiting friend-of-friend relationships among email correspondents.

%Heuristic filters: The features that are rare in normal messages but appear frequently in spam,
%	such as non- existing domain names and spam-related keywords,
%	can be used to distinguish spam from normal email.
%SpamAssassin [3] is such an example.
%Each received message is verified against the heuristic filtering rules.
%Compared with a predefined threshold,
%	the verification result decides whether the message is spam or not.

%Machine learning based filters: Since spam detection can be converted into the problem of text classification,
%	many content-based filters utilize machine-learning algorithms for filtering spam.
%Among them,
%	Bayesian-based approaches [15,19,24,35] have achieved outstanding accuracy and have been widely used.
%As these filters can adapt their classification engines with the change of message con- tent,
%	they outperform heuristic filters.

%\subsubsection{Non-content-based Techniques}

%The techniques in this sub-category use non-content spam characteristics,
%	such as source IP address,
%	message send- ing rate,
%	and violation of SMTP standards,
%	to detect email spam.
%	
%DNSBLs: DNSBLs are distributed blacklists,
%	which record IP addresses of spam sources and are accessed via DNS queries.
%When an SMTP connection is being established,
%	the receiving MTA (Mail Transfer Agent) can verify the sending machine’s IP address by querying the subscribed DNSBL.
%Even DNSBLs have been widely used,
%	their effec- tiveness [22,
%	28] and responsiveness [27] are still under study.

%MARID: MARID (MTA Authorization Records In DNS) is a class of techniques to counter forged email addresses by enforcing sender authentication.
%MARID is also based on DNS and can be seen as a distributed whitelist of authorized MTAs.
%Multiple MARID drafts [10] have been proposed,
%	in which [8,
%	12] have been deployed in some places.

%Challenge-Response (C-R): C-R is used to keep the merit of whitelist without losing important messages.
%In- coming messages,
%	whose sender email addresses are not in the recipient’s whitelist,
%	are bounced back with a challenge that needs to be solved by a human being.
%After a proper response is received,
%	the sender’s address can be added into the whitelist.
%	
%Tempfailing: Tempfailing [30] is based on the fact that legitimate SMTP servers have implemented the retry mech- anism as required by SMTP,
%	but a spammer seldom retries if sending fails.
%It usually works with a greylist that records the failed messages and the MTAs failed on their first tries.

%Delaying: As a variation of rate limiting,
%	delaying is triggered by an unusually high sending rate.
%Most delaying mechanisms,
%	such as [17,20,33,34] are applied at receiving MTAs.
%	
%Sender Behavior Analysis: This technique distinguishes spam from normal email by examining behavior of incoming SMTP connections.
%Messages from the machine exhibiting characteristics of malicious behavior such as directory har- vest are blocked before reaching mailbox [4].

%\subsubsection*{Sender-oriented Techniques}

%Usage Regulation: To effectively throttle spam at the source,
%	ISPs and ESPs (Email Service Providers) have taken various measures such as blocking port 25,
%	SMTP authentication,
%	to regulate the usage of email services.
%Message submission protocol [9] has been proposed to replace SMTP,
%	when a message is submitted from an MUA (Mail User Agent) to its MTA.

%Cost-based approaches: Borrowing the idea of postage from regular mail systems,
%	many cost-based anti-Spam techniques [5,11,14,23,32] attempt to shift the cost of thwarting spam from receiver side to sender side.
%All these techniques assume that the average email cost for a normal user is trivial and negligible,
%	but the accumulative charge for a spammer will be high enough to drive them out of business.
%Cost concept may have different forms in different proposals.
%Bonded Sender [5] advocates associating email with real money,
%	while SHRED [23] proposes affixing electronic stamps to messages.
%Both centralized [11,
%	23] and distributed [32] cost enforcement mechanisms have been pro- posed.

%social

%Besides technological solutions,

% A Study of Online Social Network Privacy Via the TAPE Framework

%Privacy in OSN have attracted many attentions.
%OSN service providers allow users to manage who can access which information (e.g. in Facebook and Google+).
%Researcher studied privacy protection from two directions. Along the first direction, fundamental changes to the current design of OSN were suggested to enhance users' privacy.
%For example, ..

%The second direction is developing privacy protection tools based on existing OSNs.
%In this section we will focus on the second direction to deal with current Information and communications technology (ICT)
%For example, ... \cite{evans_survey_2008}
%While most social network information diffusion models consider the impact of nodes and links together \cite{agarwal}

%Ostra: Leveraging trust to thwart unwanted communication
%Online communication media such as email, instant messaging, bulletin boards, voice-over-IP, and social networking sites allow any sender to reach potentially millions of users at near zero marginal 


%\subsubsection{Application Layer}

%The second class of prior work aims to understand app behavior using dynamic analysis of app binaries in controlled environments.

%Nowadays, the Web converged over two main protocols,
%	namely HTTP/HTTPS and SMTP/SMTPS
%Beside these,
%	DNS still has a central role for reaching almost any Web server.

%HTTP is the king of Web protocols,
%	and it is used almost by all services on the Internet.
%It does not include encryption and,
%	thus,
%	all its headers are transmitted in clear.
%Therefore,
%	all the details of HTTP transactions are offered to passive monitoring: an eavesdropper can read the URL and the full content of each document.
%Moreover,
%	additional headers such as Content-Type and User-Agent can provide meaningful information about the users’ setup.
%Parameters of the web pages are transmitted in clear as well,
%	and thus,
%	username and password might be extracted from packets.
%To overcome privacy risks of HTTP,
%	many services rely nowadays on HTTPS,
%	that secures the former by putting it on the top of TLS.
%No HTTP header is transmitted in clear,
%	leaving a passive monitor without any information about the underlying transaction.
%Nevertheless,
%	TLS includes a field called Server Name Indication (SNI),
%	where the client indicates in clear the domain name of the server being contacted.
%Thus,
%	the hostname of the contacted server is exposed to passive monitoring,
%	unveiling in most cases the service accessed by the user [17].
%Guidelines of TLS version 1.3 plan to encrypt this field,
%	but this poses several technical challenges,
%	as the handshake procedure would result more complicated and require additional RTTs before establishing the connection.
%Moreover,
%	many works showed the power of machine learning for extracting knowledge from encrypted connections,
%	such as webpage URL [18] and users’ QoE [19],[20].

%The same considerations hold for the DNS protocol,
%	where domain names are exchanged in plain text.
%Even DNSSEC does not guarantee confidentiality,
%	but only provides origin authentication of the DNS data.
%DNS traffic can be used to provide fine-grained visibility even with encrypted traffic by rebuilding clients DNS cache and inspecting consequent TCP/UDP flows [21],[22].

%Finally,
%	to overcome the stiffness of TCP and TLS,
%	some Content Providers designed and implemented their own protocol suites.

%This is the case of QUIC,
%	designed by Google and implemented in Google Chrome,
%	Android smartphones as well as on Google Web servers.
%It relies on UDP and provides authentication and confidentiality for HTTP transactions.
%Nevertheless,
%	the server hostname is transmitted in clear,
%	posing the same privacy issues of TLS.
%Moreover,
%	QUIC transmits client’s User-Agent in clear,
%	unveiling user’s device type.
%In addition to Google,
%	Facebook designed its own application layer protocol,
%	called Zero.
%It is based on QUIC’s crypto module,
%	but relies on TCP instead of UDP.
%It is implemented in Facebook and Instagram Web servers as well as on the mobile applications of such platforms.
%Being by design similar to QUIC,
%	the same considerations hold: the server hostname is exchanged in clear,
%	leaking to passive monitors the name of the contacted service.

%Behavioral

%Identifying Spam Without Peeking at the Contents
We consider two types of behavior models in this work,
	one based on typical behavior from a user perspective and one on typical behavior of a message.
Both of these models can be quantified into a probability score.

%user perspective
The user's behavior is modeled with respect to their typical email usage,
	the frequency and type of messages received and sent,
	and the typical recipients with whom they exchange messages.
Known as a behavior profile,
	this type of model is computed over some training period to learn how the user behaves within the email account.

These behavior models have been shown to be effective at detecting viral email propagations [24,25,26].
The measurements of the user's email behavior include the frequency of inbound/outbound email traffic,
	the specific times emails arrive and are sent,
	the "social cliques" of a user,
	and the user's response rates when replying to specific senders.

%message perspective

The second type of behavior is specific to how spam behaves and how it appears in the message folder among normal emails.
In general,
	spam emails can be spotted at a glance because they appear anomalous with respect to the normal set of emails received and opened by the user. 


