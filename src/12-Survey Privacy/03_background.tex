\section{Background}

The study of privacy measurement applied to information and communication technology (ICT) is very wide and embraces many fields of knowledge
	from Sociology and Statistics to Cryptography and Artificial Intelligence.
To better understand the privacy issues in ICT,
	we describe different types of privacy threats used by attackers to access users' private information through messaging services.
Next,
	we present four options available to the end users to deal with ICT privacy issues.

%Stalking Online: on User Privacy in Social Networks
\subsection{Privacy threats}

Four types of privacy threats have been discovered in the literature:

%Private information disclosure
\subsubsection{Private information disclosure}

When users share information with trusted social network community.
	they implicitly assume that their information shared through messages would stay within the community destination.
However,
	this assumption is not always valid,
	an individual messages may be accessed by adversaries.
For instance,
	messages sent to an email-based social network may be stored at a repository and consequently visible to the public,
	malicious users and applications may follow people through social networks,
	add-ons and third parties may access users’ private information,
	etc.

\cite{krishnamurthy_leakage_2010} shows that online social networks and applications leak users’ personally identifiable information to third parties.
The results of their study clearly show that the indirect leakage of PII via OSN identifiers to third-party aggregation servers is happening.
OSNs in our study consistently demonstrate leakage of user identifier information to one or more third-parties via Request-URIs,
	Referer headers and cookies.
In addition,
	two of the OSNs directly leak pieces of PII to third parties with one of the OSNs leaking zip code and email information about users that may not be even publicly available within the OSN itself.

%Information aggregation
\subsubsection{Information aggregation}

When users authenticate to their favorite service messaging;
	generally,
	online social networks,
	they voluntarily release different types of personal information: name,
	screen name,
	telephone numbers,
	email addresses,
	locations,
	etc.
Moreover,
	when users post messages in forums, blogs and webmails,
	they also disclose small pieces of private information.
However,
	with the development of information retrieval techniques,
	private information of the same user may be collected from different sources and aggregated to reveal user privacy \cite{luo_protecting_2009}.
	
%Inference attacks
\subsubsection{Inference attacks}

Aside from voluntary disclosure of explicit personal information,
	users information could be inferred from public information items.
The privacy literature recognizes two types of private information leakage:
	identity leakage and attribute leakage,
	and identity leakage often leads to attribute leakage.
Identity disclosure occurs when the adversary is able to determine the mapping from a record to a specific real-world entity (e.g. an individual).
Attribute disclosure occurs when an adversary is able to determine the value of a user attribute that the user intended to stay private.

%\cite{he_inferring_2005} study a type of indirect private information inference through social relations.
%They notice that hidden attributes could be inferred from friends’ attributes using a Bayesian network.
%They study the factors that impact inference accuracy,
%	and suggest that selectively hiding social connections or friends’ attributes could help preserve privacy.
%	
%Privacy Threats in Published Social Network Data
\subsubsection{Re-identification and De-anonymization attacks}

When social network data sets are published for various legitimate reasons,
	user identity and some profile information are often removed to protect the user privacy.
The privacy literature recognizes two types of privacy mechanisms:
	interactive and non-interactive.
In the interactive mechanism,
	an adversary poses queries to a data-base and the database provider gives noisy answers.
In the non-interactive setting,
	a data provider releases an anonymized version of the database to meet privacy concerns.
Some of the well-known techniques for this purpose includes k-anonymity,
	l-diversity and t-closeness \cite{li_tcloseness_2017}.
For instance,
	in a k-anonymized data set,
	an individual cannot be distinguished by attributes from other k-1 records.
However,
	possibilities of \textbf{attribute re-identification attacks} on publicly available data sets have been studied in \cite{li_new_2014}.
They show that user identities could be recovered from anonymized data sets.

On the other hand,
	due to the nature of social network data,
	just anonymizing node attributes is not enough.
Graph structure contains significant amount of information which could be utilized to hurt user privacy,
	i.e. \textbf{structural reidentification attacks}.
A good survey on structural anonymization and re-identification attacks could be found at \cite{zhou_brief_2008}.
	most works show that node identities could be inferred through graph structure.

\subsection{Privacy solutions}

%Privacy as a Product: A Case Study in the m-Health Sector
When an ICT user accesses a service,
	in particular,
	a messaging service,
	she has to share some information with the service provider,
	namely identity,
	type of service required,
	location,
	etc.
Clearly,
	the shared information depends on the service but regardless of the exchanged information,
	to deal with the existing privacy issues, users have to choose between four main options:
%	(1) trust the provider and send him all the required information,
%	(2) individually protect the data sent to the untrusted provider,
%	(3) collaborate with the provider to protect her data,
%	or (4) collaborate with other users to protect their data from the provider.

%Privacy based on trust
\subsubsection{Privacy based on trust}
This is probably the most common situation.
Users tend to trust service providers because,
	they do not really have alternatives in many cases.
Due to the fact that privacy is considered a right,
	most countries have regulations that oblige companies to guarantee the privacy of their users.
Among this regulation,
	when users data are released to third parties they should be sanitized so as to guarantee users privacy \cite{chen_privacypreserving_2009}.
%	statistical disclosure control techniques are generally used for sanitization.

%Privacy based on Individual
\subsubsection{Privacy based on Individual User Actions}\label{sssec:num2}
Despite the legislation,
	users might prefer to keep some of their private information away from the service provider.
In this case,
	we assume that the user cannot collaborate with the service provider.
For example,
	in the case of sending a query to an Internet search engine such as Google or Yahoo.
The user cannot initiate a \textbf{collaborative protocol} with the search engine,
	because the search engine is only able to receive and answer queries.

%Privacy based on Collaboration with the Provider
\subsubsection{Privacy based on Collaboration with the Provider}
There are situations in which the service provider might collaborate with the user to protect her privacy by running \textbf{privacy-aware protocols.}
TODO

%Privacy based on Collaboration with other Users
\subsubsection{Privacy based on Collaboration with other Users}
This is an evolution of the proposals described in \ref{sssec:num2} in which users collaborate to protect their privacy.
In this case,
	users do not want to trust the provider nor other third parties.
TODO





